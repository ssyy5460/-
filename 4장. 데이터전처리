데이터의 품질과 데이터에 담긴 유용한 정보의 양은 머신러닝 알고리즘을 얼마나 잘 학습할 수 있는지 결정하는 중요한 요소

- 데이터셋에서 누락된 값을 제거하거나 대체하기
- 범주형 데이터 변환하기
- 모델과 관련이 높은 특성 선택하기


4. 누락된 데이터 다루기
일반적으로 누락된 값은 테이블에 빈 공간이나 예약된 용어(nan: Not a Number, Null )을 사용

4.1 누락된 값이 있는 샘플이나 특성 제외
dropna() : 누락된 값이 있는 행 or 열 제거
너무 많은 데이터를 제거하면 정보가 부족해 모델이 학습하는데 어려움을 가질 수 있음

4.2 누락된 값 대체
Imputer 클래스를 사용해 평균값이나 최빈값으로 대체

4.3 사이킷런 추정기 
추정기 : 분류기는 변환기 클래스와 개념상 매우 유사한 API를 가짐
- predict와 transform의 메서드를 가짐
- 새로운 데이터에 대해 예측을 만듦

변환기
- 학습한 파라미터로 데이터를 변환
- 특성 개수가 같아야함

4.2 범주형 데이터 다루기
4.2.1 순서가 있는 특성과 순서가 있는 특성
순서가 없는 특성(color) , 순서가 있는 특성(size), 수치형 특성(price)이 있음
순서가 있는 레이블을 다루는 순서를 가진 분류 또는 순서를 가진 회귀라고도 부름

4.2.2. 순서 특성 매핑
학습 알고리즘이 순서 특성을 올바르게 인식하려면 범주형의 문자열 값을 정수로 바꿔줘야한다.
나중에 정수값을 원래 문자열 표현으로 바꾸고 싶다면 간단히 거꾸로 매핑하는 딕셔러릴를 정의하면 된다.

4.2.3 클래스 레이블 인코딩
사이킷런의 분류 추정기 ㅣ대부분은 자체저긍로 클래스 레이블을 정수로 변환해 주지만 사소서한 실수를 방지하기 위해 클래스 레이블을 정수 배열로 전달하는 것이 좋은 습관임
fit_transform() 메서드는 fit 메서드와 transform 메서드를 합쳐놓은 단축 메서드임
inverse_transform() 메서드를사용하면 정수 클래스 레이블을 원본 문자형 형태로 되돌릴 수 있음

4.2.4 순서가 없는 특성에 원-핫 인코딩 적용
원-핫 인코딩 : 순서 없는 특성에 들어 있는 고유한 값마다 새로운 더미 특성을 만드는 것
기본적으로 원핫인코딩은 희소행렬을 반환함
원핫 인코딩으로 더미변수를 만들기 위한 편한 방법은 get_dummies()를 사용하는 것
주의해야할 점은 다중 공선성 문제. 역행렬을 구해야하는 경우 상관관계가 높으면 역행렬을 계산하기 어려워 수치적으로 불안정해짐
변수 간의 상관관계를 제거하기 위해 원-핫 인코딩된 배열에서 특성 열을 하나 삭제ㅔ한다. 이렇게 특성을 삭제해도 잃는 정보는 없다.

4.3 데이터 세트를 훈련 세트와 테스트 세트로 나누기
train_test_split()
stratify = y : 클래스 레이블 배열 y을 전달하면 훈련 세트와 테스트 세트에 있는 클래스 비율이 원본 데이터터셋과 동일하게 유지됨
데이터셋을 훈련 세트와 테스트 세트로 나누게 되면 학습 알고리즘에 도움이 될 수 있는 유익한 정보를 감추게 됨
테스트 세트가 작으면 일반화 오차에 대한 추정이 부정확해서 트레이드오프의 균형을 맞추어야함
6:4,7:3,8:2로 나누며 대용량의 데이터의 경우 9:1, 99:1로 나눔
떼어놓았던 테스트 세트를 버리지 말고 훈련과 평가 후에 전체 데이터셋을 모델을 다시 훈련하여 모델의 예층 성능을 향상시키는 방법이 사용됨
일반적이라면 권장이되지만, 데이터셋이 작고 이상치가 들어있다면 오히려 일반화 성능은 나쁠 수 있음

4.4 특성 스케일 맞추기
특성 스케일에 둔감한 랜덤포레스트

1.정규화(MinMax)
특성의 스케일을 [0,1] 범위에 맞품
각 특성의 열마다 최소-최대 스케일 변환을 적용하여 정규화함
범위가 정해진 값이 필요할 때 유용하게 사용되는 기법
2.표준화(Standard)
머신러닝 알고리즘, 특히 경사하강법 같은 최적화 알고리즘에 많이 사용됨
표준화를 사용하면 평균을 0에 맞추고 표준편차를 1로 만들어 정규분포와 같은 특징을 만들로록 함
가중치를 더 쉽게 학습할 수 있음, 이상치 정보가 유지되기 때문에 제한된 범위로 조정하는 최대-최소 변환에 비해 이상치의 영향을 덜 받음. 

4.5 유용한 특성선택
모델이 테스트 세트보다 훈련 세트에서 성능이 높다면 과대적합의 신호임
과대적합은 모델 파라미터가 훈련 세트에 있는 특정 샘플들에 너무 가깝게 맞추어져 있어 새로운 데이터에서는 일반화하지 못하는 경향이있어 분산이 크게 나타남

1. 더 많은 훈련 데이터셋 수집
2. 규제를 통해 복잡도 제한
3. 파라미터 개수가 적은 간단한 모델 선택
4. 데이터 차원을 줄임

4.5.1 모델 복잡도 제한을 위한 L1 규제(라쏘)와 L2 규제(릿지)
L1 : 가중치의 절댓값
L2 : 가중치의 제곱

가중치 제곱을 그냥 가중치 절댓값으로 바꾼것. L2 규제와 대조적으로 L1 규제는 보통 희소한 특성 벡터를 만듦.
대부분의 특성 가중치가 0이 됨. 실제로 관련 없는 특성이 많은 고차원 데이터셋일 경우 이런 희소성이 도움이 될 수 있음.
특히 샘플보다 관련 없는 특성이 더 많은 경우. L1규제는 특성 선택의 기법이 될 수 있음

4.5.2 L2 규제의 기하학적 해석
L2규제는 비용 함수에 페널티 항을 추가함. 등고선 원
규제가 없는 비용 함수로 훈련한 모델에 비해 가중치 값을 아주 작게 만드는 효과를 냄
목표 : 훈련 데이터에서 비용 함수를 최소화하는 가중치 값의 조합을 찾은 것임
규제(더 작은 가중치)를 얻기 위해 비용 함수에 추가하는 페널티항으로 큰 자중치를 제한함
규제의 강도를 크게 하면 가중치가 0에 가까워지고 훈련 데이터에 대한 모델 의존성은 줄어듦
-> 우리의 목표는 규제가 없는 비용과 페널티 항의 합을 최소화하는 것.
   모델을 학습할만한 충분한 데이터가 없을 때 편향을 추가하여 모델을 간단하게 만듦으로써 분산은 줄이는 것으로 봄

4.5.3 L1 규제를 사용한 희소성
L1 규제의 등고선은 날카롭기 때문에 비용 함수의 포물선과 L1 다이아몬드의 경계가 만나는 최적점은 축에 가깝게 위치할 가능성이 높음
규제 강도에 따라 특성의 가중치 변화가 다름

4.5.4 순차 특성 선택 알고리즘
모델의 복잡도를 줄이고 과대적합을 줄이는 방법으로 차원축소가 있음
특성 선택은 원본 일부에서 선택
특성 추출은 일련의 특성에서 얻은 정보로 새로운 특성 선택

순차 특성 선택 알고리즘은 탐욕적 탐색 알고리즘으로 초기 d차원의 특성 공간을 k차원의 특성 부분공간으로 축소함
주어진 문제에 가장 관련이 높은 특성 부분 집합을 자동으로 선택하는 것이 목적
관계없느 특성이나 잡음을 제거하여 계산 효율성을 높이고 모델 일반화 오차를 줄임 규제를 제공하지 않는 알고리즘을 선택할 때 유용

순차 후진 선택은 계산 효율성을 향상하기 위해 모델 성능을 가능한 적게 희생하면서 초기 특성의 부분공간으로 축소함.
과대적합을 알고 있는 모델이라면 예측성능을 높일 수 있음

SBS는 새로운 특성의 부분 공간이 목표로 하는 특성 갯수가 될 때까지 전체 특성에서 순차적으로 특성을 제거함 
각 단계에서 어떤 특성을 제거할지 판단하기 위해 최소화할 기준함수를 정의함
기준함수에서 계산한 값은 어떡 특성을 제거하기 전후의 모델 성능 차이임
각 단계에서 제거했을 때 성능 손실이 최대가 되는 특성을 제거함

ex. SBS 알고리즘을 사용하여 최적의 부분 집합에 대해 모으고 분류기의 정확도 그래프를 그려보면 정확도가 100이 나오는 부분이 있음
SBS 알고리즘이 최적의 부분집합을 찾았다는 의미-> 차원의 저주가 감소함
전체 특성 조합을 사용한 데이터셋과 최적 부분집합의 데이터셋이 같은 성능을 보인다.
-> 데이터셋을 훈련 세트와 테스트 세트로 나눈 것과 다시 훈련 세트와 검증 세트로 나눈 방식에 영향을 받음
-> 특정 개수를 줄여 KNN의 성능이 증가하지 않았지만 데이터셋 크기를 줄였습니다.
-> 데이터 수집비용이 높은 실전 애플리케이션에서는 유용할 듯
-> 특성 개수를 크게 줄여 간단한 모델과 해석하기 쉬움.

4.6 랜덤포레스트의 특성 중요도 사용
데이터의 유용한 특성을 선택하는 방법
앙상블에 참여한 모든 결정 트리에서 계산한 평균적인 불순도 감소로 특성 중요도를 선택할 수 있음
선형적으로 구분이 가능한지 가정할 필요가 없음 
트리 기반 모델은 표준화나 정규화할 필요가 없음
