데이터의 품질과 데이터에 담긴 유용한 정보의 양은 머신러닝 알고리즘을 얼마나 잘 학습할 수 있는지 결정하는 중요한 요소

- 데이터셋에서 누락된 값을 제거하거나 대체하기
- 범주형 데이터 변환하기
- 모델과 관련이 높은 특성 선택하기


4. 누락된 데이터 다루기
일반적으로 누락된 값은 테이블에 빈 공간이나 예약된 용어(nan: Not a Number, Null )을 사용

4.1 누락된 값이 있는 샘플이나 특성 제외
dropna() : 누락된 값이 있는 행 or 열 제거
너무 많은 데이터를 제거하면 정보가 부족해 모델이 학습하는데 어려움을 가질 수 있음

4.2 누락된 값 대체
Imputer 클래스를 사용해 평균값이나 최빈값으로 대체

4.3 사이킷런 추정기 익히기정기
추정기 : 분류기는 변환기 클래스와 개념상 매우 유사한 API를 가짐
- predict와 transform의 메서드를 가짐
- 새로운 데이터에 대해 예측을 만듦
변환기
- 학습한 파라미터로 데이터를 변환
- 특성 개수가 같아야함

4.2 범주형 데이터 다루기
4.2.1 순서가 있는 특성과 순서가 있는 특성
순서가 없는 특성(color) , 순서가 있는 특성(size), 수치형 특성(price)이 있음
순서가 있는 레이블을 다루는 순서를 가진 분류 또는 순서를 가진 회귀라고도 부름

4.2.2. 순서 특성 매핑
학습 알고리즘이 순서 특성을 올바르게 인식하려면 범주형의 문자열 값을 정수로 바꿔줘야한다.
나중에 정수값을 원래 문자열 표현으로 바꾸고 싶다면 간단히 거꾸로 매핑하는 딕셔러릴를 정의하면 된다.

4.2.3 클래스 레이블 인코딩
사이킷런의 분류 추정기 ㅣ대부분은 자체저긍로 클래스 레이블을 정수로 변환해 주지만 사소서한 실수를 방지하기 위해 클래스 레이블을 정수 배열로 전달하는 것이 좋은 습관임
fit_transform() 메서드는 fit 메서드와 transform 메서드를 합쳐놓은 단축 메서드임
inverse_transform() 메서드를사용하면 정수 클래스 레이블을 원본 문자형 형태로 되돌릴 수 있음

4.2.4 순서가 없는 특성에 원-핫 인코딩 적용
원-핫 인코딩 : 순서 없는 특성에 들어 있는 고유한 값마다 새로운 더미 특성을 만드는 것
기본적으로 원핫인코딩은 희소행렬을 반환함
원핫 인코딩으로 더미변수를 만들기 위한 편한 방버은 get_dummies()를 사용하는 것
주의해야할 점은 다중 공선성 문제. 역행렬을 구해야하는 경우 ㅌ상관관계가 높으면 역행렬을 계산하기 어려워 수치적으로 불안정해짐
변수 간이 상관관계를 제거하기 위해 원-핫 인코딩된 배열에서 특성 열을 하나 삭제ㅔ한다. 이렇게 특성을 삭제해도 잃는 정보는 없다.

4.3 데이터 세트를 훈련 세트와 테스트 세트로 나누기
train_test_split()
stratify = y : 클래스 레이블 배열 y을 전달하면 훈련 세트와 테스트 세트에 있는 클래스 비율이 원본 데이터터셋과 동일하게 유지됨
데이터셋을 훈련 세트와 테스트 세트로 나누게 되면 학습 알고리즘에 도움이 될 수 있는 유익한 정보를 감추게 됨
테스트 세트가 작으면 일반화 오차에 대한 추정이 부정확해서 트레이드오프의 균형을 맞추어야함
6:4,7:3,8:2로 나누며 대용량의 데이터의 경우 9:1, 99:1로 나눔
떼어놓았던 테스트 세트를 버리지 말고 훈련과 평가 후에 전체 데이터셋을 모델을 다시 훈련하여 모델의 예층 성능을 향상시키는 방법이 사용됨
일반적이라면 권장이되지만, 데이터셋이 작고 이상치가 들어있다면 오히려 일반화 성능은 나쁠 수 있음

4.4 특성 스케일 맞추기
특성 스케일에 둔감한 랜덤포레스트

1.정규화(MinMax)
특성의 스케일을 [0,1] 범위에 맞품
각 특성의 열마다 최소-최대 스케일 변환을 적용하여 정규화함
범위가 정해진 값이 필요할 때 유용하게 사용되는 기법
2.표준화(Standard)
머신러닝 알고리즘, 특히 경사하강법 같은 최적화 알고리즘에 많이 사용됨
표준화를 사용하면 평균을 0에 맞추고 표준편차를 1로 만들어 정규분포와 같은 특징을 만들로록 함
가중치를 더 쉽게 학습할 수 있음, 이상치 정보가 유지되기 때문에 제한된 범위로 조정하는 최대-최소 변환에 비해 이상치의 영향을 덜 받음.
