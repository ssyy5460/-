특정 문제에 알맞은 분류 알고리즘을 선택하려면 연습이 필요함.
알고리즘은 저마다 특징이 있고 일정한 가정을 전제로 함. 
데이비드 월퍼트의 공짜 점심 없음 이론을 되새겨 보면 모든 경우에 뛰어난 성능을 낼 수 있는 분류 모델은 없음.
-> 어느 한 task에 최적화된 머신러닝 모델은 다른 task에 적용한다고 한들 제대로 task 수행하기를 바라는 것은
공짜 점심을 바라는 것과 같다
최소한 몇 개의 학습 알고리즘 성능을 비교하고 해당 문제에 최선인 모델을 선택하는 것이 권장됨
특성이나 샘플의 개수에 따라 다르고 데이터셋에 있는 잡음 데이터의 양과 클래스가 선형으로 구분되는지 아닌지에 따라 다름.
-> 분류모델은 예측성능과 계산 성능은 학습에 사용하려는 데이터에 크게 의존함

머신러닝 알고리즘을 훈련하기 위한 다섯 가지 주요 단계를 정리하면
1. 특성을 선택하고 훈련 샘플을 모은다.
2. 성능 지표를 선택한다
3. 분류 모델과 최적화 알고리즘을 선택한다.
4. 모델의 성능을 평가한다.
5. 알고리즘을 튜닝한다.

Label Encoder를 사용하는 이유?
- 사소한 실수를 피할 수 있고 작은 메모리 영ㅇ역을 차지하므로 계산 성능을 향상시키기 때문

train_test_split(stratify = y) 계층화를 사용하는 이유?
계층화는 룬련 세트와 테스트 세트의 클래스 레이블 비율을 입력 데이터셋과 동일하게 만든다는 의미 

많은 머신러닝 알고리즘과 최적화 알고리즘은 최상의 성능을 위해 특성 스케일을 조정함
훈련 세트와 테스트 세트의 샘플이 서로 같은 비율로 이동되도록 동일한 평균과 표준편차를 사용하여 테스트 세트를 표준화한다.

# 퍼셉트론으로 분류
- eta : 학습률
- max_iter : 에포크 횟수
학습률이 너무 크면 전역최솟값을 지나치고 학습률이 너무 작으면 학습 속도가 느리기때문에 수렴하는데 오래 걸림
선형적으로 구분되지 않는 데이터 세트에 수렴하지 못함. 실전에서 분류알고리즘에 퍼셉트론을 사용하지 않는 이유
에포크마다 적어도 하나의 샘플이 잘못 분류되기 때문에 가중치 업데이트가 끝도 없이 계속됨
학습률을 바꾸거나 에프크 횟수를 늘릴 수 있지만 퍼셉트론은 데이터셋에 절대 수렴하지 못함

# 로지스틱 회귀를 사용한 클래스 확률 모델링
- 구현하기 쉽고 선형적으로 구분된느 클래스에 뛰어난 성능을 분류 모델
- 산업계에서 널리 쓰이는 분류 알고리즘 중 하나.
- 선형 모델이지만 퍼셉트론이나 아달린과 마찬가지로 다중 분류로 확장이 가능하다

오즈비 : 특정 이벤트가 발생활 확률
 P / (1-P)
 
 logit(P) = log(P/1-P)
 
logit함수를 이용해 0과 1사이의 입력값을 받아 실수 범위 값으로 변환한다.

특성 가중치 합과 로그 오즈 사이의 선형 관계를 다음과 같이 사용할 수 있다.
logit(P(y=1|x)) = w0x0 + ... + wmxm
특성 x가 주어졌을 때 샘플이 1에 속할 조건부 확률

어떤 샘플이 특정 클래스에 속할 확률을 예측하는 것이 관심 대상이므로 logit 함수를 거꾸로 뒤집음
-> 로지스틱 시그모이드 함수 = 시그모이드 함수

# 아달린과 로지스틱 회귀의 차이점
아달린 : 선형 활성화 함수 사용
로지스틱 회귀 : 시그모이드 활성화 함수 사용

실제로 클래스를 예측하는 것보다클래스에 소속될 확률을 추정하는 것이 유용한 경우도 있다
ex. 비가 오는 예측뿐만 아니라 비 올 확률을 예측해야하는 날씨 예보
    비슷하게 어떤 증상이 있는 환자가 특정 질병을 가질 확률 예측
    
로지스틱 비용 함수의 가중치 학습(모델 파라미터 w를 어떻게 학습하는지 살펴보기)
아달린 분류 모델은 함수를 최소화하려는 가중치 w를 학습함.
-> 로그 가능도를 최대화하는 것이 앞서 정의한 비용 함수를 최소화하는 것과 동일하기 때문에 경사 하강법 업데이트 규칙과 동일함

로지스틱 회귀 비용 함수를 유도하는 방법을 설명하기 위해 로지스틱 회귀모델을 만들 때 최대화하려는 
가능도(우도) 정의 필요 : 데이터 세트에 있는 각 샘플이 서로 독립적
우도 공식의 로그를 최대화하는 것이 쉬움 -> 로그 가능도 함수
로그 가능도 함수를 적용하면 가능도가 매우 작을 때 발생하는 수치상의 언더플로를 미연에 방지한다
둘째, 계수의 곱을 계수의 합으로 바꿀 수 있다
경사 상승법같은 최적화 알고리즘을 사용하여 로그 가능도 함수를 최대화할 수 있다.

시그모이드 활성화 대비 로지스틱 비용 그래프를 살펴본 결과
클래스 1에 속한 샘플을 정확히 예측하면 비용이 0에 가까워진다.
마찬가지로 클래스 0에 속한 샘플을 정확히 예측하면 y축의 비용이 0에 가까워진다.
예측이 잘못되면 비용이 무한대가 된다. 잘못된 예측에 점점 더 큰 비용을 부여함

# 아달린 구현을 로지스틱 회귀 알고리즘으로 변경
로지스틱 회귀를 구현하려면 아달린 구현에서 비용 함수를 새로운 비용함수로 바꾸기만 하면됨
선형 활성화함수를 시그모이드 활성화로 바꾸고 임계 함수가 클래스 레이블 -1과 1이 아니고 0과 1을 반환하도록 변경 
세가지만 바꾸면 로지스틱 회귀 알고리즘으로 변경 가능

# 규제를 사용하여 과대적합 피하기
과대적합은 모델이 훈련 데이터로는 잘 동작하지만 테스트 데이터로는 일반화되지 않는 현상으로 
모델이 과대적합일 때 분산이 크다고 말한다. 비슷하게 모델이 과소적합일 때 편향이 크다고 말한다.
훈련 데이터에 있는 패턴을 감지할 정도로 충분히 모델이 복잡하지 않다는 것을 의미하며 새로운 데이터에서도 성능이 낮다.

분산 : 모델을 여러 번 훈련했을 때 특정 샘플에 대한 예측의 일관성을 측정함
훈련 데이터 세트의 일부분을 사용하여 여러 번 훈련하는 경우. 모델이 훈련 데이터의 무작위성에 민감함
편향 : 다른 훈련 데이터셋에서 여러 번 훈련했을 때 예측이 정확한 값에서 얼마나 벗어났는 지 측정. 
무작위성이 아니라 구조적인 에러를 나타냄

좋은 편향-분산 트레이드오프를 찾는 한 가지 방법은 규제를 사용하여 모델의 복잡도를 조정하는 것임.
규제는 공선성을 다루거나 데이터에서 잡음을 제거하여 과대적합을 방지할 수 있는 매우 유용한 방법이다.
규제는 과도한 파라미터(가중치) 값을 제한하기 위해 추가적인 정보(편향)를 주입하는 개념으로
L2규제(라쏘)가 많이 쓰임

로지스틱 회귀의 비용함수는 규제 항을 추가해 모델 훈련 과정에서 가중치를 줄이는 역할을 함
규제 파라미터 람다를 사용하여 가중치를 작게 유지하면서 훈련 데이터에 얼마나 잘 맞출지를 조정할 수 있음

사이킷런의 매개변수 C는 서포트 벡터 머신 형식에서 따왔으며 규제 하이퍼 파라미터의 역수임
결과적으로 역 규제 파라미터를 C값을 감수시키면 규제 강도가 증가함
매개변수 C가 감수하면 가중치 절댓값이 줄어듦. 즉, 규제강도가 증가함.

# 서포트 벡터 머신을 사용한 최대 마진 분류
강력하고 널리 사용되는 학습 알고리즘으로 퍼셉트론의 확장버전임.
퍼셉트론 알고리즘은 분류 오차를 최소화하고, SVM은 마친을 최대화하여 최적화함
마진은 클래스를 구분하는 초평면과 이 초평면에 가장 가까운 훈련 샘플 사이의 거리로 정의함.-> 서포트벡터

큰 마진의 결정 경계를 원하는 이유는 일반화 오차가 낮아지는 경향이 있기 때문임


# 슬랙 변수를 사용하여 비선형 분류 문제 다루기
소프트 마진 분류 : 슬랙변수를 사용하여 선형적으로 구부뇌지 않는 데이터에서 선형 제약 조건을 완화하기 위해 도입됨
-> 적절히 비용을 손해 보면서 분류 오차가 있는 상황에서 최적화 알고리즘이 수렴함

매개변수 C를 사용하여 마진 폭을 제어할 수 있고 편향-분산의 트레이드 오프를 조정함
C값이 크면 오차에 대한 비용이 커지고, 작으면 분류 오차에 덜 엄격해진다.

로지스틱 회귀 VS 서포트 벡터 머신
실제 분류 작업에서 선형 로지스틱 회귀와 선형 SVM은 종종 매우 비슷한 결과를 만듦. 로지스틱 회귀는 훈련 데이터의 조건부 가능도를 최대화하기 때문에
SVM보다 이상치에 민감함. SVM은 결정 경계에 가장 가까운 포인트에 대부분 관심을 둠. 로지스틱 회귀는 모델이 간단하고 구현하기가 더 쉽고, 업데이트가 용이하므로
스트리밍 데이터를 다룰 때 적합함

# 커널 SVM을 사용하여 비선형 문제 풀기
선형적으로 구분되지 않는 데이터를 다루는 커널 방법의 기본 아이디어는 매핑 함수를 사용하여 원본 특성의 비선형 조합을 선형적으로 구분되는 고차원 공간에 투영하는 것이다.
2차원 데이터 세트를 투영을 통해 새로운 3차원 특성 공간으로 변환하면 클래스를 구분할 수 있음

# 커널 기법을 사용하여 고차원 공간에서 분할 초평면 찾기
SVM으로 비선형 문제를 풀기 위해 매핑 함수를 사용하여 훈련 데이터를 고차원 특성 공간으로 변환함. 그다음 새로운 특성 공간에서 데이터를 분류하는 선형 SVM 모델을 훈련함.
동일한 매핑 함수를 사용하여 새로운 본 적 없는 데이터를 변환하고 선형 SVM 모델을 사용항 분류할 수 있음

이런 매핑 방식은 새로운 특성을 만드는 계산 비용이 매우 비쌈. 
두 포인트 사이 점곱을 계산하는 데 드는 높은 비용을 절감하기 위해 커널 함수를 정의함. 가장 널리 사용되는 커널 중 하나는 방사 기저 함수로 가우시안 커널이라고도 함
커널이란 샘플간의 유사도 함수로 해석할 수 있음
분류기에 따라 훈련 데이터에 잘 맞지만 본 적 없는 데이터에서는 일반화 오차가 높을 것임
감마 매개 변수가 과대적합을 조절하는 중요한 역할을 하는 것을 볼 수 있음

# 의사결정나무
의사결정 분류기는 설명이 중요할 때 유용한 모델이다. 일련의 질문에 대한 결정을 통해 데이터를 분해함
트리의 루트에서 시작해서 정보 이득이 최대가 되는 특성으로 데이터를 나눈다. 
반복과정을 통해 리프 노드가 순수해질 때까지 모든 자식 노드에서 해당 과정을 반복함
실제로 이렇게 하면 노드가 많은 깊은 트리가 만들어지고 과대적합될 가능성이 높아짐 -> 가지치기를 통해 트리의 최대 깊이 제한

## 정보 이득 최대화 : 자원을 최대한 이용
가장 정보가 풍부한 특성으로 노드를 나누기 위해 트리 알고리즘으로 최적화할 목적 함수를 정의하여 각 분할에서 정보 이득을 최대화한다.
자식 노드의 불순도가 낮을 수록 정보이득이 커짐. 이진 결정 트리에 널리 사용되는 세 개의 불순도 지표 또는 분할 조건은 지니 불순도, 엔트로피, 분류 오차가 있음

엔트로피 : 클래스 분포가 균등하면 엔트로피는 최대가 됨
예를 들어 p1 = 0.5 , p0 = 0.5 으로 균등하게 분포되어있으면 엔트로피는 1이 됨.
엔트로피의 조건을 트리의 상호 의존 정보를 최대화하는 것으로 이해할 수 있음

지니 불순도 : 잘못 분류될 확률을 최소화하기 위한 기준으로 이해함
지니 불순도는 클래스가 완벽하게 섞여 있을 때 최대가 됨

지니불순도와 엔트로피 모두 비슷한 결과가 나타나 불순도 조건을 바꾸어 평가하는 것보다 가지치기 수준을 바꾸면서 튜닝하는 편이 훨씬 나음

분류 오차 : 가지치기에는 좋은 기준이지만 결정 트리를 구성하는 데 권장되지 않음. 노드의 클래스 확률 변화에 덜 민감하기 때문


# 랜덤 포레스트로 여러 개의 의사결정나무 연결
결정트리의 앙상블 : 여러 개의 결정 트리를 평균 내는 것으로 개개인의 트리는 분산이 높은 문제가 있지만 앙상블은 견고한 모델을 만들어 일반화 성능을 높이고
과대적합의 위험을 줄임

1. n개의 랜덤한 부트스트램 샘플을 뽑음(훈련 세트에서 중복을 허용하면서 랜덤하게 n개의 샘플을 선택함)
2. 부트스트랩 샘플에서 결정 트리를 학습함
  a. 중복을 허용하지 않고 랜덤하게 d개의 특성을 선택함
  b. 정보 이득과 같은 목적함수를 기준으로 최선의 분할을 만드는 특성을 사용해 노드를 분할함
3. 단계 1,2를 K번 반복함
4. 각 트리의 예측을 모아 다수결 투표로 클래스 레이블을 할당함

중복을 허용한 샘플링은 샘플이 독립적이고 공분상이 0임

랜덤포레스트는 결정 트리만큼 해석이 쉽지 않지만 하이퍼파라미터 튜닝에 많은 노력을 기울이지 않아도 되는게 장점
랜덤포레스트는 가지치기할 필요가 없으며 앙상블 모델이 개별 결정 트리가 만든느 잡음으로부터 매우 안정되어있기 때문

실전에서 신경 써야 할 파라미터는 랜덤 포레스트가 만들 트리 개수 하나.
트리 개수가 많아질 수록 계산비용이 증가하는 만큼 랜덤 포레스트 분류기의 성능이 좋아짐
부트스트랩 샘플의 크기 n을 사용하면 랜덤 포레스트의 편향 - 분산 트레이드오프를 조절할 수 있습니다.
부트스트랩 크기가 작아지면 개별 트리의 다양성이 증가합니다. 특정 훈련 샘플이 부트스트랩 샘플에 포함될 확률이 낮기 때문

사이킷런을 포함한 대부분의 라이브러리에서 부트스트랩 샘플 크기를 원본 훈련 세트의 샘플 개수와 동일 하게해야함

# k-최근접 이웃 : 게으른 학습 알고리즘
전형적인 게으른 학습기로 훈련 데이터에서 판별함수를 학습하는 대신 훈련 데이터세트를 메모리에 저장하기 위해서임

1. 숫자 k와 거리 측정 기준을 선택함
2. 분류하려는 샘플에서 k개의 최근접 이웃을 찾음
3. 다수결 투표를 통해 클래스 레이블을 할당함

이런 메모리 기반 방식의 분류기는 수집된 새로운 훈련 데이터셋에 바로 적용할 수 있다는 장점이 있지만 새로운 샘플을 분류하는 계산 복잡도는 단점임
훈련 단계가 없기 떄문에 훈련 샘플을 버릴 수 없으며, 최악의 경우 훈련 데이터셋의 샘플개수에 선형적으로 증가함.

적절한 k값을 선택하는 것은 과대적합과 과소적합 사이에서 올바른 균형을 잡기 위해 중ㅇ함
실수값을 가진 특성에는 간단한 유클리디안 거리를 사용하고, 동일하게 취급되도록 표준화를 진행해야함

모수 vs 비모수모델
머신러닝 알고리즘은 모수 모델과 비모수 모델로 묶을 수 있음.
모수 모델은 새로운 데이터 포인트를 분류할 수 있는 함수를 학습하기 위해 훈련 데이터셋에서 파라미터를 추정함.
훈련이 끝나면 원본 훈련 데이터셋이 더 이상 필요하지 않음 -> 선형 로지스틱 모델, 선형 SVM, 퍼셉트론

비모수모델은 고정된 개수의 파라미터로 설며오딜 수 없으며 훈련 데이터가 늘어남에 따라 파라미터도 늘어나게 됨 -> 의사결정나무,랜덤포레스트, 커널 SVM

KNN은 비모수모델에 속하며 인스턴스 기반 모델로 훈련 데이터셋을 메모리에 저장한다.
게으른 학습은 인스턴스 기반 학습의 특별한 경우이며 학습과정에 비용이 들지 않음



차원의 저주
KNN은 차원의 저주 떄문에 과대적합되기 쉬움. 고정된 크기의 훈련 데이터셋이 차원이 늘어남에 따라 특성 공간이 점점 희소해지는 것을 말함.
고차원의 공간에서는 가장 가까운 이웃이라도 좋은 추정값을 만들기에는 너무 멀리 떨어져 있다는 뜻
결정 트리나 KNN같이 규제를 적용할 수 없는 모델에서는 차원축소나 특성 선택을 사용하면 차원의 저주를 피하는데 도움이 됨
